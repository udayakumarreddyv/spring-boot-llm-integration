spring:
  application:
    name: llm-integration-service
  
server:
  port: 8080

# LLM Provider Configurations
llm:
  providers:
    openai:
      enabled: true
      base-url: "https://api.openai.com/v1"
      api-key: "${OPENAI_API_KEY:}"
      default-model: "gpt-3.5-turbo"
      timeout: 30000
      
    anthropic:
      enabled: true
      base-url: "https://api.anthropic.com/v1"
      api-key: "${ANTHROPIC_API_KEY:}"
      default-model: "claude-3-sonnet-20240229"
      timeout: 30000
      
    ollama:
      enabled: true
      base-url: "http://localhost:11434/v1"
      api-key: "ollama"
      default-model: "llama2"
      timeout: 60000
      
    azure-openai:
      enabled: false
      base-url: "${AZURE_OPENAI_ENDPOINT:}"
      api-key: "${AZURE_OPENAI_API_KEY:}"
      default-model: "gpt-35-turbo"
      timeout: 30000
      
    huggingface:
      enabled: false
      base-url: "https://api-inference.huggingface.co/models"
      api-key: "${HUGGINGFACE_API_KEY:}"
      default-model: "microsoft/DialoGPT-medium"
      timeout: 30000

  # Global settings
  default-provider: "openai"
  max-tokens: 1000
  temperature: 0.7
  stream: false

# Actuator configuration
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics
  endpoint:
    health:
      show-details: when-authorized

# Logging configuration
logging:
  level:
    com.example.llm: DEBUG
    org.springframework.web: INFO
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %msg%n"
